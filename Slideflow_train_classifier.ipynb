{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and evaluating classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: You cannot take advantage of parallel processing with GPUs from within a Jupyter notebook so tasks requiring or benefitting from multiple GPUs should use .py scripts instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1: Training with a Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should have your Project set up already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. import libraries & set vars\n",
    "import os\n",
    "import slideflow as sf\n",
    "os.environ['SF_BACKEND'] = 'torch'\n",
    "\n",
    "# 2. initialize project\n",
    "project_root = '/home/pearsonlab/DATA/PROJECTS/TEST_PROJECT'\n",
    "P = sf.Project(root=project_root)\n",
    "\n",
    "# 3. set hyperparameters\n",
    "hp = sf.ModelParams(\n",
    "    tile_px=299, \n",
    "    tile_um=302, \n",
    "    epochs=[5], # will save a model and results after each epoch in the list (i.e. [1,3,5]) \n",
    "    toplayer_epochs=0, \n",
    "    model='xception', \n",
    "    pooling='avg',\n",
    "    loss='sparse_categorical_crossentropy', \n",
    "    learning_rate=0.0001, \n",
    "    learning_rate_decay=0.98,\n",
    "    learning_rate_decay_steps=512, \n",
    "    batch_size=64, # typically you want as large of a batch size as you can fit in memory\n",
    "    hidden_layers=1, \n",
    "    hidden_layer_width=500,\n",
    "    optimizer='Adam', \n",
    "    early_stop=True, # prevents overfitting, highly recommended\n",
    "    early_stop_patience=0, # you can set what epoch you want to allow for early stopping\n",
    "    early_stop_method='accuracy',\n",
    "    manual_early_stop_epochs=None,\n",
    "    manual_early_stop_batch=None,\n",
    "    training_balance='category', \n",
    "    validation_balance='none', \n",
    "    trainable_layers=0, \n",
    "    l1=0, \n",
    "    l2=0, \n",
    "    l1_dense=None,\n",
    "    l2_dense=None, \n",
    "    dropout=0.2, \n",
    "    uq=False, # uncertainty quantification, adds much more time to training\n",
    "    augment='xyrjb', # all random augmentations: x=horizontal flip, y=vertical flip, r=rotate, j=jpeg compression, b=Gaussian blur, n=Stain Normalizer \n",
    "    normalizer=None, # 'reinhard_fast' is usually best\n",
    "    normalizer_source=None,\n",
    "    include_top=False, \n",
    "    drop_images=False)\n",
    "\n",
    "# 4. train model, will save in project models/ folder\n",
    "results = P.train(\n",
    "    outcomes=\"tumor_type\",\n",
    "    exp_label='test_tutorial', # this will become part of the name of the resulting trained model folder. Saved model name will be format \"00001-exp_label-outcome-epoch1\"\n",
    "    filters={\"dataset\": [\"train\"], \"exclude\": [\"no\"]},\n",
    "    pretrain='imagenet',\n",
    "    save_predictions=True, \n",
    "    params=hp,\n",
    "    # if doing k-fold cross validation\n",
    "    val_strategy='k-fold',\n",
    "    val_k_fold=3,\n",
    "    validate_on_batch=100 # this determines how often a validation step occurs. You can mess with this to get better early stopping results, depends on size of dataset\n",
    "    )\n",
    "\n",
    "# 5. evaluate model, will save in project eval/ folder\n",
    "P.evaluate(\n",
    "    model=\"/path/to/trained_model_epoch1\",\n",
    "    outcomes=\"tumor_type\",\n",
    "    filters={\"dataset\": [\"test\"]}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can provide a dataset to the ```P.train()``` instead of using filters on the Project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = P.dataset(tile_px=299, tile_um=302)\n",
    "dataset = dataset.filter({\"exclude\": [\"no\"]})\n",
    "\n",
    "# Option 1: use filters\n",
    "train_dataset = dataset.filter({\"dataset\": [\"train\"]})\n",
    "val_dataset = dataset.filter({\"dataset\": [\"val\"]})\n",
    "test_dataset = dataset.filter({\"dataset\": [\"test\"]})\n",
    "\n",
    "# Option 2: use split\n",
    "train_dataset, val_dataset, test_dataset = dataset.split(\n",
    "    train=0.8, val=0.1, test=0.1\n",
    ")\n",
    "\n",
    "# Train\n",
    "results = P.train(\n",
    "    outcomes=\"tumor_type\",\n",
    "    params=hp,\n",
    "    dataset=train_dataset,\n",
    "    val_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "P.evaluate(\n",
    "  model=\"/path/to/trained_model_epoch1\",\n",
    "  outcomes=\"tumor_type\",\n",
    "  dataset=test_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Walkthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 2: Training with a Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are still bugs with this but we are working on it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a dataset\n",
    "import os\n",
    "import slideflow as sf\n",
    "from pprint import pprint\n",
    "os.environ['SF_BACKEND'] = 'torch'\n",
    "tiles_savedir = '/home/pearsonlab/PROJECTS/CHRONIC_ENDOMETRITIS/data/cells/extracted_cells_tiles/'\n",
    "tfr_savedir = '/home/pearsonlab/PROJECTS/CHRONIC_ENDOMETRITIS/data/cells/extracted_cells_tfr/ALL/'\n",
    "tile_px = 96\n",
    "tile_um = '40x'\n",
    "tile_file_format = 'jpg'\n",
    "\n",
    "# Create a dataset\n",
    "dataset = sf.Dataset(\n",
    "    config='/home/pearsonlab/PROJECTS/CHRONIC_ENDOMETRITIS/data/cells/cell_classifier/datasets.json',\n",
    "    sources=['UCH_ENDOMETRITIS_EVAL_YOLO', 'UCH_ENDOMETRITIS_NEG_YOLO', 'UCH_ENDOMETRITIS_LOW_YOLO'],\n",
    "    annotations='/home/pearsonlab/PROJECTS/CHRONIC_ENDOMETRITIS/bennett_plasmacells_anns.csv',\n",
    "    # filters={'group': 'eval'}, # You can provide filters to the dataset or you can \n",
    "    tile_px=tile_px,\n",
    "    tile_um=tile_um,\n",
    ")\n",
    "dataset.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get labels\n",
    "labels, unique_labels = dataset.labels('cell_class')\n",
    "\n",
    "# prep dataset splits\n",
    "# NOTE I was splitting them randomly but that would split the patients up and I may get no negatives in my validation set and it was a problem\n",
    "(train_ds, val_ds) = dataset.filter({'group': 'train'}).split(model_type='categorical', labels='cell_class', val_strategy='fixed', val_fraction=0.3)\n",
    "train_ds = dataset.filter({'patient': ['NPC_Control_1', 'PC_Control_1', 'NPC_Control_2', 'PC_Control_2']})\n",
    "val_ds = dataset.filter({'patient': ['NPC_Control_3', 'PC_Control_3', 'NPC_Control_4', 'PC_Control_4']})\n",
    "test_ds = dataset.filter({'group': 'test'})\n",
    "eval_ds = dataset.filter({'slide': ['LowP1', 'LowP2', 'LowP3', 'LowP4']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import basename\n",
    "models_dir = '/home/pearsonlab/PROJECTS/CHRONIC_ENDOMETRITIS/data/cells/cell_classifier/models'\n",
    "eval_dir = '/home/pearsonlab/PROJECTS/CHRONIC_ENDOMETRITIS/data/cells/cell_classifier/eval'\n",
    "model_name = \"test_5e_bs64_vob40_uq\"\n",
    "exp_label = \"Lows\"\n",
    "outdir = f'{eval_dir}/eval-{model_name}-{exp_label}'\n",
    "model_path = '/home/pearsonlab/PROJECTS/CHRONIC_ENDOMETRITIS/data/cells/cell_classifier/models/test_5e_bs64_vob40_uq/test_5e_bs64_vob40_uq_epoch4'\n",
    "\n",
    "# read in params.json and edit to add in the info we need\n",
    "config = sf.util.get_model_config(model_path)\n",
    "config['outcomes'] = ['cell_class']\n",
    "config['outcome_labels'] = {\"0\": \"NPC\", \"1\": \"PC\"}\n",
    "config[\"input_features\"] = None\n",
    "config[\"input_feature_sizes\"] = None\n",
    "config[\"input_feature_labels\"] = None\n",
    "config[\"model_type\"] = \"categorical\"\n",
    "sf.util.write_json(config, os.path.join(model_path, 'params.json'))\n",
    "# also save in the higher directory\n",
    "sf.util.write_json(config, os.path.join('/home/pearsonlab/PROJECTS/CHRONIC_ENDOMETRITIS/data/cells/cell_classifier/models/test_5e_bs64_vob40_uq', 'params.json'))\n",
    "\n",
    "# Build trainer instead & then load model\n",
    "config = sf.util.get_model_config(model_path)\n",
    "hp = sf.ModelParams.from_dict(config['hp'])\n",
    "trainer = sf.model.Trainer(hp=hp,\n",
    "                 outdir=outdir,\n",
    "                 labels=labels,\n",
    "                 outcome_names=['cell_class'],\n",
    "                 name=f\"eval-{model_name}-{exp_label}\",\n",
    "                 config=config,\n",
    "                 load_method='weights',\n",
    "                 )\n",
    "trainer.load(model_path)\n",
    "\n",
    "# Evaluate\n",
    "results_dict = trainer.evaluate(eval_ds,\n",
    "                                batch_size=64,\n",
    "                                save_predictions='csv',\n",
    "                                uq=True,\n",
    "                                )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
