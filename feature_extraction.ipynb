{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slideflow Tutorial Notebook: Feature Extraction, UMAP Visualization, MIL Classifier\n",
    "This notebook is designed to take users through:<br>\n",
    "1. [Importing a Slideflow Project](#import)\n",
    "    - [Tile Extraction](#tile)<br>\n",
    "2. [Feature Extraction](#fe)\n",
    "    - [Pre-Trained Feature Extraction](#pre-fe)<br>\n",
    "3. [Feature Visualization](#feature_viz)\n",
    "    - [UMAP Visualization](#umap_viz)<br>\n",
    "4. [Training an SSL model Feature Extraction](#ssl-fe)<br><br>\n",
    "5. [Training an MIL Model](#mil) *Not done* <br><br>\n",
    "6. [Classifier Result Visualization](#class_viz) *Not done* \n",
    "    - [Custom AUROC](#auroc)\n",
    "    - [Confusion Matrix](#cmat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='import'></a>\n",
    "### Import libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Always the first step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variables with os package\n",
    "import os\n",
    "os.environ['SF_BACKEND'] = 'torch' # Alternative is 'tensorflow'\n",
    "os.environ['SF_SLIDE_BACKEND'] = 'cucim' # Alternative is 'libvips'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0' # Set which GPU(s) to use \n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1' # Make sure CUDA kernel doesn't asynchronously start\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = '1' # Extra CUDA error logging\n",
    "\n",
    "# Check if GPU is available\n",
    "if os.environ['SF_BACKEND']=='torch':\n",
    "    import torch\n",
    "    print('GPU available: ', torch.cuda.is_available())\n",
    "    print('GPU count: ', torch.cuda.device_count())\n",
    "    print('GPU current: ', torch.cuda.current_device())\n",
    "    print('GPU name: ', torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "elif os.environ['SF_BACKEND']=='tensorflow':\n",
    "    import tensorflow as tf\n",
    "    print(\"GPU: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "# import slideflow\n",
    "import slideflow as sf\n",
    "\n",
    "# Set verbose logging\n",
    "import logging\n",
    "logging.getLogger('slideflow').setLevel(logging.INFO)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '10'\n",
    "import sys\n",
    "sys.stderr = sys.__stdout__\n",
    "\n",
    "# Check if slideflow was properly installed\n",
    "sf.about()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='project'></a>\n",
    "### Getting Started with a Slideflow Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial assumes that you have already created a project folder. Once the project has been created and you have specified the paths to datasets, annotation files, etc. we will begin by initializing a Slideflow Project object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set root paths\n",
    "username = \"skochanny\" # change me\n",
    "root_path = f'/scratch/{username}/PROJECTS'\n",
    "labshare_path = '/gpfs/data/pearson-lab/'\n",
    "project_name = \"TEST_PROJECT\"\n",
    "project_root_path = f\"{root_path}/{project_name}\"\n",
    "\n",
    "# # Initialize the Project class object\n",
    "P = sf.Project(project_root_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='tile'></a>\n",
    "#### Tile Extraction (If Needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the project and dataset have been specified, we can extract tiles from the whole slide images. If this step has already been completed, you can skip this section.<br><br>\n",
    "Notes regarding tile extraction:<br>\n",
    "- If you do not have manually annotated ROIs, the image will be tiled. If taking this route, be sure to edit the `whitespace_fraction` parameter such that tiles that are mostly white space are not included in the analysis. You can check the extraction report to see this. \n",
    "- If you do have manually annotated ROIs, you can force the tile extration to be only within the ROIs by setting `roi_method='inside'` \n",
    "- `qc` is a quality control parameter that will remove tiles based on blur detection and/or otsu's method<br>\n",
    "\n",
    "Additional parameters for tile extraction can be found in the [Slideflow documentation here](https://slideflow.dev/dataset/#slideflow.Dataset.extract_tiles). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from slideflow.slide import qc\n",
    "P.extract_tiles(tile_px=224, # 224x224 pixel tiles are default for most feature extractors\n",
    "                tile_um=2, \n",
    "                whitespace_fraction=0.95,\n",
    "                roi_method='inside',\n",
    "                source = ['LUADvsLUSC'],\n",
    "                qc=[qc.Gaussian(), qc.Otsu()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='fe'></a>\n",
    "## Feature Extraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create/load the dataset\n",
    "2. Create a feature extractor object (either [SSL](#ssl-feature-extraction-model-training) or [Pre-Trained](#pre-trained-feature-extraction))\n",
    "3. Calculate and export features\n",
    "\n",
    "See [Generating features](https://slideflow.dev/features/) in Slideflow's documentation for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To initialize a [Dataset object](https://slideflow.dev/dataset/), you can use the Project object's `dataset()` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "ad_sq = P.dataset(tile_px=224, tile_um=224)\n",
    "\n",
    "# Get a summary of the dataset\n",
    "ad_sq.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some situations, you may want to perform analysis only on a subset of images within a single dataset. You can filter a dataset by a specific feature of the annotation file with `Dataset.filter()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by site. Sites included in the filter will be included in the dataset\n",
    "# This is an example of filtering, for the purposes of this tutorial, we will not filter\n",
    "filter_ad_sq = ad_sq.filter({'site': ['Site-97', 'Site-40', 'Site-9', 'Site-177', 'Site-130', 'Side-69', 'Site-67', 'Site-93', 'Site-96']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Trained Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-trained feature extraction is very fast! This is because no model training occurs during feature extraction. The model has already been trained and we are simply passing our images through this network to extract features. \n",
    "\n",
    "[Here is the list](https://slideflow.dev/features/#pretrained-extractors) of all available pre-trained feature extractors in Slideflow. Some are not available in the base ```slideflow``` package because they require additional dependencies allowing for non-commercial options. You can install the ```slideflow-noncommercial``` package to get access to these.\n",
    "\n",
    "Some notes:\n",
    "- We can access the extracted features by initializing a [DatasetFeatures](https://slideflow.dev/dataset_features/) class object. \n",
    "- It is fine to extract tiles at a different resolution than what is needed for feature extraction (usually 224px). The feature extractor will resize the tiles to the correct resolution with the `resize` parameter.\n",
    "- After you have generated the features aka feature bags, you want to save them as torch tensors to access later for MIL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set extractor\n",
    "extractor_name = 'virchow'\n",
    "# Define a path for a .pkl file to save the features to:\n",
    "cache_name = f'{project_root_path}/features/{extractor_name}.pkl'\n",
    "\n",
    "print(sf.model.list_extractors()) # to see all available feature extractors\n",
    "\n",
    "# Create the feature extractor (uncomment the extractor you would like to use)\n",
    "# phikon = build_feature_extractor('phikon', tile_px=224)\n",
    "# plip = build_feature_extractor('plip', tile_px=224)\n",
    "# ctranspath = build_feature_extractor('ctranspath', tile_px=224)\n",
    "# retccl = build_feature_extractor('retccl', tile_px=224)\n",
    "extractor = sf.model.build_feature_extractor('virchow', \n",
    "                                           tile_px=224, \n",
    "                                           weights=f'{labshare_path}/MODELS/{extractor_name}/{extractor_name}_model.bin') # or whatever the actual path is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Option 1*: how to generate & save features (can sometimes have memory issue to store entire DatasetsFeatures object in memory, see Option 2):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the dataset features object\n",
    "features = sf.DatasetFeatures(extractor, ad_sq, resize=True, cache=cache_name) # you can also use a normalizer on the features: normalizer='reinhard'\n",
    "\n",
    "# Save feature bags as torch tensors to access them later for MIL\n",
    "features.to_torch(project_root_path + f'/features/{extractor_name}/torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: you can either save features at /features or at /pt_files\n",
    "# for train_mil(bags=...)\n",
    "# bags (str) – Either a path to directory with *.pt files, or a list of paths to \n",
    "# individual *.pt files. Each file should contain exported feature vectors, \n",
    "# with each file containing all tile features for one patient.\n",
    "# bags=project_root_path + f'/features/{extractor_name}/torch'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "source": [
    "*Option 2*: at a project level, you can also generate the features and save them in one step with `P.generate_feature_bags()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate & export feature bags.\n",
    "P.generate_feature_bags(extractor, ad_sq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UMAP Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is useful to visualize the features in a lower-dimensional space. We can use UMAP for this. We'll load in features, calculate UMAP activations, and save. \n",
    "\n",
    "Also see Slideflow documentation on [feature space analysis](https://slideflow.dev/posthoc/) for more info and techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = sf.DatasetFeatures(extractor, ad_sq, cache=f'{project_root_path}/features/{extractor_name}.pkl')\n",
    "umap_cache = f'{project_root_path}/cohort/umap'\n",
    "umap = features.map_activations(n_neighbors=15, min_dist=0.5)\n",
    "labels, _ = ad_sq.labels('cohort', format='name')\n",
    "print(labels)\n",
    "umap.label_by_slide(labels)\n",
    "umap.save(umap_cache, \n",
    "        xlabel='UMAP1',\n",
    "        ylabel='UMAP2',\n",
    "        title = f'{extractor_name} Features LUAD vs. LUSC',\n",
    "        legend='Subtype',\n",
    "        subsample=5000,\n",
    "        s=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in UMAP activations as Slideflow [SlideMap](https://slideflow.dev/slidemap/) object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap = sf.SlideMap.load(f'{project_root_path}/umap/cohort')\n",
    "save_path = f'{project_root_path}/umap/site/umap_site.png'\n",
    "labels, _ = ad_sq.labels('site', format='name')\n",
    "print(labels)\n",
    "umap.label_by_slide(labels)\n",
    "umap.save_plot(save_path,\n",
    "                xlabel='UMAP1',\n",
    "                ylabel='UMAP2',\n",
    "                title= f'{extractor_name} Features LUAD vs. LUSC by Site',\n",
    "                legend='Subtype',\n",
    "                subsample=12000,\n",
    "                loc='upper right',\n",
    "                ncol=2,\n",
    "                legend_kwargs={'fontsize':8,'markerscale':0.7,'frameon':True,'title_fontsize':10},\n",
    "                s=6)\n",
    "\n",
    "# legend_kwargs={'fontsize':8,'markerscale':0.7,'frameon':False,'title_fontsize':10},"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom SSL Feature Extraction Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: Training an SSL model requires big GPUs and a lot of data & memory, you probably cannot do it in a Jupyter Notebook. This is only here so you know this functionality exists.*\n",
    "\n",
    "SSL feature extraction requires first building a model. This model tries to learn information about the dataset without any human labels; then, we are able to use this model as our now pre-trained featuer extractor. Currently, Slideflow is integrated with two SSL models, SimCLR and DinoV2.<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DinoV2 Overview <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Install DinoV2\n",
    "2. Create a DinoV2 config file\n",
    "3. Train DinoV2 with the commandline interface\n",
    "\n",
    "DinoV2 requires a `config.yaml` file to be specified. Here is an example of what this file should look like:\n",
    "```\n",
    "\n",
    "train:\n",
    "  dataset_path: slideflow\n",
    "  batch_size_per_gpu: 32\n",
    "  slideflow:\n",
    "    project: \"/mnt/data/projects/TCGA_THCA_BRAF\"\n",
    "    dataset:\n",
    "      tile_px: 299\n",
    "      tile_um: 302\n",
    "      filters:\n",
    "        brs_class:\n",
    "        - \"Braf-like\"\n",
    "        - \"Ras-like\"\n",
    "    seed: 42\n",
    "    outcome_labels: \"brs_class\"\n",
    "    normalizer: \"reinhard_mask\"\n",
    "    interleave_kwargs: null\n",
    "```\n",
    "More information on DinoV2 configuration can be found [here](https://slideflow.dev/ssl/#training-dinov2)<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download DinoV2\n",
    "! pip install git+https://github.com/jamesdolezal/dinov2.git\n",
    "# OR you can clone the repository "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a YAML file, modifying the contents as needed: <br>\n",
    "```\n",
    "train:\n",
    "  dataset_path: slideflow\n",
    "  batch_size_per_gpu: 32\n",
    "  slideflow:\n",
    "    project: \"/home/ecdyer/labshare/DL_OTHER/TEST_PROJECTS/TEST_PROJECT/\"\n",
    "    dataset:\n",
    "      tile_px: 299\n",
    "      tile_um: 302\n",
    "      filters:\n",
    "        cohort:\n",
    "        - \"LUSC\"\n",
    "        - \"LUAD\"\n",
    "    seed: 42\n",
    "    outcome_labels: \"cohort\"\n",
    "    normalizer: \"reinhard_mask\"\n",
    "    interleave_kwargs: null\n",
    "```\n",
    "<br><br>\n",
    "Train DinoV2 with the commandline interface:<br>\n",
    "```\n",
    "torchrun --nproc_per_node=4 -m \"dinov2.train.train\" \\\n",
    "    --config-file /path/to/config.yaml \\\n",
    "    --output-dir /path/to/output_dir\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SimCLR Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: Training SimCLR requires tensorflow.*\n",
    "1. Define SimCLR parameters\n",
    "2. Train SimCLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simclr_args = simclr.get_args(\n",
    "    mode='train',\n",
    "    train_mode='pretrain',\n",
    "    train_batch_size=256,\n",
    "    temperature=0.1,\n",
    "    learning_rate=0.075,\n",
    "    learning_rate_scaling='sqrt',\n",
    "    weight_decay=1e-4,\n",
    "    train_epochs=100,\n",
    "    image_size=299,\n",
    "    checkpoint_epochs=20)\n",
    "\n",
    "P.train_simclr(simclr_args, ad_sq, normalizer = 'reinhard_mask')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once models have trained, you can build a feature extractor for each model and extract the features. To save the features, define a path to save a `.pkl`file with the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SimCLR Feature Extraction\n",
    "cache_name = f'{project_root_path}/features/simclr.pkl'\n",
    "\n",
    "simclr_extractor = build_feature_extractor('simclr', ckpt=f'{project_root_path}/simclr/00006-simclr/ckpt-116420.ckpt')\n",
    "simclr_features = sf.DatasetFeatures(simclr_extractor, ad_sq, cache=cache_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dinov2 Feature Extraction\n",
    "cache_name = f'{project_root_path}/features/dinov2.pkl'\n",
    "dinov2 = build_feature_extractor(\n",
    "    'dinov2',\n",
    "    weights='/path/to/teacher_checkpoint.pth',\n",
    "    cfg='/path/to/config.yaml')\n",
    "dinov2_features = sf.DatasetFeatures(dinov2, ad_sq, cache=cache_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
